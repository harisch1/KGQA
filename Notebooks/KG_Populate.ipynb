{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U pip setuptools wheel\n",
        "%pip install -U 'spacy[cuda112]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsF7EAzXIXNL"
      },
      "outputs": [],
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U 'spacy[cuda112]'\n",
        "!pip install py2neo\n",
        "!pip install wikipedia\n",
        "# !pip install spacy==3.2.4\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "# !pip install jupyterlab\n",
        "!pip install neo4j\n",
        "# !pip install pywikibot\n",
        "!pip install sklearn\n",
        "\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwB-Hd6LIXNP",
        "outputId": "50f507f6-94c2-4bb3-be6b-51c69f97a8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.2.4\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import urllib\n",
        "from pprint import pprint\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
        "from py2neo.bulk import merge_nodes\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "print(spacy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWXxlEkzaDK2",
        "outputId": "94078f31-c10d-42fe-d75b-98fae37a49e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\haris\\Downloads\\PolyU\\Capstone Project\\KGQA\\Notebooks\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "directory = os.getcwd()\n",
        "\n",
        "print(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntn-g55vIXNR",
        "outputId": "b035c85d-399a-4a8f-d091-5c5a31e22d19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function spacy.pipeline.functions.merge_noun_chunks(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "VERBS = ['ROOT', 'advcl']\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
        "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
        "\n",
        "api_key = open('../keys/.api_key').read()\n",
        "\n",
        "non_nc = spacy.load('en_core_web_lg')\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "nlp.add_pipe('merge_noun_chunks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HbHF5PEzXCDr"
      },
      "outputs": [],
      "source": [
        "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
        "    \n",
        "    text_ls = []\n",
        "    node_label_ls = []\n",
        "    url_ls = []\n",
        "    \n",
        "    params = {\n",
        "        'query': query,\n",
        "        'limit': limit,\n",
        "        'indent': indent,\n",
        "        'key': api_key,\n",
        "    }   \n",
        "    \n",
        "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
        "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
        "    response = json.loads(urllib.request.urlopen(url).read())\n",
        "    \n",
        "    if return_lists:\n",
        "        for element in response['itemListElement']:\n",
        "\n",
        "            try:\n",
        "                node_label_ls.append(element['result']['@type'])\n",
        "            except:\n",
        "                node_label_ls.append('')\n",
        "\n",
        "            try:\n",
        "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
        "            except:\n",
        "                text_ls.append('')\n",
        "                \n",
        "            try:\n",
        "                url_ls.append(element['result']['detailedDescription']['url'])\n",
        "            except:\n",
        "                url_ls.append('')\n",
        "                \n",
        "        return text_ls, node_label_ls, url_ls\n",
        "    \n",
        "    else:\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8jtebDVFIXNT"
      },
      "outputs": [],
      "source": [
        "def remove_special_characters(text):\n",
        "    \n",
        "    regex = re.compile(r'[\\n\\r\\t]')\n",
        "    clean_text = regex.sub(\" \", text)\n",
        "    \n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def remove_stop_words_and_punct(text, print_text=False):\n",
        "    \n",
        "    result_ls = []\n",
        "    rsw_doc = non_nc(text)\n",
        "    \n",
        "    for token in rsw_doc:\n",
        "        if print_text:\n",
        "            print(token, token.is_stop)\n",
        "            print('--------------')\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            result_ls.append(str(token))\n",
        "    \n",
        "    result_str = ' '.join(result_ls)\n",
        "\n",
        "    return result_str\n",
        "\n",
        "\n",
        "def create_svo_lists(doc, print_lists):\n",
        "    \n",
        "    subject_ls = []\n",
        "    verb_ls = []\n",
        "    object_ls = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ in SUBJECTS:\n",
        "            subject_ls.append((token.lower_, token.idx))\n",
        "        elif token.dep_ in VERBS:\n",
        "            verb_ls.append((token.lemma_, token.idx))\n",
        "        elif token.dep_ in OBJECTS:\n",
        "            object_ls.append((token.lower_, token.idx))\n",
        "\n",
        "    if print_lists:\n",
        "        print('SUBJECTS: ', subject_ls)\n",
        "        print('VERBS: ', verb_ls)\n",
        "        print('OBJECTS: ', object_ls)\n",
        "    \n",
        "    return subject_ls, verb_ls, object_ls\n",
        "\n",
        "\n",
        "def remove_duplicates(tup, tup_posn):\n",
        "    \n",
        "    check_val = set()\n",
        "    result = []\n",
        "    \n",
        "    for i in tup:\n",
        "        if i[tup_posn] not in check_val:\n",
        "            result.append(i)\n",
        "            check_val.add(i[tup_posn])\n",
        "            \n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_dates(tup_ls):\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    for entry in tup_ls:\n",
        "        if not entry[2].isdigit():\n",
        "            clean_tup_ls.append(entry)\n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_svo_triples(text, print_lists=False):\n",
        "    \n",
        "    clean_text = remove_special_characters(text)\n",
        "    doc = nlp(clean_text)\n",
        "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
        "    \n",
        "    graph_tup_ls = []\n",
        "    dedup_tup_ls = []\n",
        "    clean_tup_ls = []\n",
        "\n",
        "    for subj in subject_ls: \n",
        "        for obj in object_ls:\n",
        "            \n",
        "            dist_ls = []\n",
        "            \n",
        "            for v in verb_ls:\n",
        "                \n",
        "                # Assemble a list of distances between each object and each verb\n",
        "                dist_ls.append(abs(obj[1] - v[1]))\n",
        "                \n",
        "            # Get the index of the verb with the smallest distance to the object \n",
        "            # and return that verb\n",
        "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
        "            \n",
        "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
        "            # later down in the process to allow for proper sentence recognition.\n",
        "\n",
        "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
        "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
        "            \n",
        "            # Add entries to the graph iff neither subject nor object is blank\n",
        "            if no_sw_subj and no_sw_obj:\n",
        "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
        "                graph_tup_ls.append(tup)\n",
        " \n",
        "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
        "\n",
        "    \n",
        "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
        "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
        "    \n",
        "    return clean_tup_ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nieKOVzzIXNV"
      },
      "outputs": [],
      "source": [
        "def get_obj_properties(tup_ls):\n",
        "    \n",
        "    init_obj_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "\n",
        "        try:\n",
        "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
        "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
        "        except:\n",
        "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
        "        \n",
        "        init_obj_tup_ls.append(new_tup)\n",
        "        \n",
        "    return init_obj_tup_ls\n",
        "\n",
        "\n",
        "def add_layer(tup_ls):\n",
        "\n",
        "    svo_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        \n",
        "        if tup[3]:\n",
        "            svo_tup = create_svo_triples(tup[3])\n",
        "            svo_tup_ls.extend(svo_tup)\n",
        "        else:\n",
        "            continue\n",
        "    \n",
        "    return get_obj_properties(svo_tup_ls)\n",
        "        \n",
        "\n",
        "def subj_equals_obj(tup_ls):\n",
        "    \n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[0] != tup[2]:\n",
        "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
        "            \n",
        "    return new_tup_ls\n",
        "\n",
        "\n",
        "def check_for_string_labels(tup_ls):\n",
        "    # This is for an edge case where the object does not get fully populated\n",
        "    # resulting in the node labels being assigned to string instead of list.\n",
        "    # This may not be strictly necessary and the lines using it are commnted out\n",
        "    # below.  Run this function if you come across this case.\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        if isinstance(el[2], list):\n",
        "            clean_tup_ls.append(el)\n",
        "            \n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_word_vectors(tup_ls):\n",
        "\n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[3]:\n",
        "            doc = nlp(tup[3])\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
        "        else:\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
        "        new_tup_ls.append(new_tup)\n",
        "        \n",
        "    return new_tup_ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1ZFcynTJIXNV"
      },
      "outputs": [],
      "source": [
        "def dedup(tup_ls):\n",
        "    \n",
        "    visited = set()\n",
        "    output_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if not tup[0] in visited:\n",
        "            visited.add(tup[0])\n",
        "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
        "            \n",
        "    return output_ls\n",
        "\n",
        "\n",
        "def convert_vec_to_ls(tup_ls):\n",
        "    \n",
        "    vec_to_ls_tup = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        vec_ls = [float(v) for v in el[4]]\n",
        "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
        "        vec_to_ls_tup.append(tup)\n",
        "        \n",
        "    return vec_to_ls_tup\n",
        "\n",
        "\n",
        "def add_nodes(tup_ls):   \n",
        "\n",
        "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
        "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
        "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
        "    \n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DAzc1QXeIXNW"
      },
      "outputs": [],
      "source": [
        "def add_edges(edge_ls):\n",
        "    \n",
        "    edge_dc = {} \n",
        "    \n",
        "    # Group tuple by verb\n",
        "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
        "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
        "    \n",
        "    for tup in edge_ls: \n",
        "        if tup[1] in edge_dc: \n",
        "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
        "        else: \n",
        "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
        "    \n",
        "    for edge_labels, tup_ls in edge_dc.items():   # k=edge labels, v = list of tuples\n",
        "        \n",
        "        tx = graph.begin()\n",
        "        \n",
        "        for el in tup_ls:\n",
        "            source_node = nodes_matcher.match(name=el[0]).first()\n",
        "            target_node = nodes_matcher.match(name=el[2]).first()\n",
        "            if not source_node:\n",
        "                source_node = Node('Node', name=el[0])\n",
        "                tx.create(source_node)\n",
        "            if not target_node:\n",
        "                try:\n",
        "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
        "                    tx.create(target_node)\n",
        "                except:\n",
        "                    continue\n",
        "            try:\n",
        "                rel = Relationship(source_node, edge_labels, target_node)\n",
        "            except:\n",
        "                continue\n",
        "            tx.create(rel)\n",
        "        tx.commit()\n",
        "    \n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bx3R-lCIIXNW"
      },
      "outputs": [],
      "source": [
        "def edge_tuple_creation(text):\n",
        "    \n",
        "    initial_tup_ls = create_svo_triples(text)\n",
        "    init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
        "    new_layer_ls = add_layer(init_obj_tup_ls)\n",
        "    starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
        "    edge_ls = subj_equals_obj(starter_edge_ls)\n",
        "    edges_word_vec_ls = create_word_vectors(edge_ls)\n",
        "    \n",
        "    return edges_word_vec_ls\n",
        "\n",
        "\n",
        "def node_tuple_creation(edges_word_vec_ls):\n",
        "    \n",
        "    orig_node_tup_ls = [(edges_word_vec_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
        "    obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
        "    full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
        "    cleaned_node_tup_ls = check_for_string_labels(full_node_tup_ls)\n",
        "    #dedup_node_tup_ls = dedup(cleaned_node_tup_ls)\n",
        "    dedup_node_tup_ls = cleaned_node_tup_ls\n",
        "    node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)\n",
        "    \n",
        "    return node_tup_ls    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z3ZaThAEIXNX"
      },
      "outputs": [],
      "source": [
        "graph = Graph('bolt://localhost:7687', auth = (\"neo4j\", \"tester\"))\n",
        "nodes_matcher = NodeMatcher(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w6aIyghEIXNX"
      },
      "outputs": [],
      "source": [
        "# text= wikipedia.summary('vincent van gogh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cPwLinhEuduF"
      },
      "outputs": [],
      "source": [
        "with open('../data/text_dump.json', 'r') as fp:\n",
        "    text_dump = json.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ac4Gjucoevv4"
      },
      "outputs": [],
      "source": [
        "def get_nodes_edges(text):\n",
        "    edges_word_vec_ls = edge_tuple_creation(text) \n",
        "    node_tup_ls = node_tuple_creation(edges_word_vec_ls)\n",
        "    return edges_word_vec_ls, node_tup_ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB8IYFtvu4kV",
        "outputId": "61e415b6-9caa-4359-d322-a6522db84cd4"
      },
      "outputs": [],
      "source": [
        "# full_edge_ls, full_node_ls = [], []\n",
        "\n",
        "for text in tqdm(text_dump.items()):\n",
        "    try: \n",
        "        edge_ls, node_ls = get_nodes_edges(text[1])\n",
        "        dedup_node_tup_ls = dedup(node_ls)\n",
        "        add_nodes(dedup_node_tup_ls)\n",
        "        add_edges(edge_ls)\n",
        "        # if full_edge_ls:\n",
        "        #     full_edge_ls += edge_ls\n",
        "        #     full_node_ls += node_ls\n",
        "        # else: \n",
        "        #     full_edge_ls = edge_ls\n",
        "        #     full_node_ls = node_ls\n",
        "    except:\n",
        "        print(f'cant do {text[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13790"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "with open ('../data/full_node_ls', 'rb') as fp:\n",
        "        full_node_ls = pickle.load(fp)\n",
        "len(full_node_ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7p-Ncrs4IXNY"
      },
      "outputs": [],
      "source": [
        "# full_edges_word_vec_ls = edge_tuple_creation(text) \n",
        "# full_node_tup_ls = node_tuple_creation(full_edges_word_vec_ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7X0DbAY1G957"
      },
      "outputs": [],
      "source": [
        "# data = dict()\n",
        "# data['full_dedup_node_tup_ls'] = full_dedup_node_tup_ls\n",
        "# data['full_edge_ls'] = full_edge_ls\n",
        "# with open('DB_dump.json', 'w') as fp:\n",
        "#     json.dump(data, fp)\n",
        "# import pickle\n",
        "\n",
        "# with open('full_dedup_node_tup_ls', 'wb') as fp:\n",
        "#     pickle.dump(full_dedup_node_tup_ls, fp)\n",
        "# fp.close()\n",
        "\n",
        "# with open('full_edge_ls', 'wb') as fp:\n",
        "#     pickle.dump(full_edge_ls, fp)\n",
        "# fp.close()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZfkt4_NIXNZ",
        "outputId": "3eb8996a-ac82-4e23-f02b-57c0fb32ad12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31918 12545\n"
          ]
        }
      ],
      "source": [
        "# full_dedup_node_tup_ls = dedup(full_node_ls)\n",
        "# print(len(full_node_ls), len(full_dedup_node_tup_ls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNxqoRoR1Tte",
        "outputId": "c770c963-2c48-4e49-fa67-166b47abf5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12545 31709\n"
          ]
        }
      ],
      "source": [
        "# print(len(full_dedup_node_tup_ls), len(full_edge_ls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IXd8FIkJJNW"
      },
      "outputs": [],
      "source": [
        "# add_nodes(full_dedup_node_tup_ls)\n",
        "# add_edges(full_edge_ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RN1WesFu8_1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "KG Populate.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "025d9167e2ac5a8645e9495aa681c87935cbdf9d0befc4bc5a72c93f41644586"
    },
    "kernelspec": {
      "display_name": "Python 3.9.11 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
